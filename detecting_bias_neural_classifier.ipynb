{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "detecting_bias_neural_classifier.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOuH1VI4gLOjyjQ772Ucp5P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drew-walkerr/DetectingBias/blob/main/detecting_bias_neural_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tutorial Links from class\n",
        "1. https://www.kaggle.com/deshwalmahesh/nlp-beginner-1-rnn-lstm-gru-embeddings-glove\n",
        "2. https://www.kaggle.com/kredy10/simple-lstm-for-text-classification \n",
        "\n",
        "3. https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/\n"
      ],
      "metadata": {
        "id": "DVY8IYo38A2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NL3u_QuiAk3h",
        "outputId": "fff7d5aa-db1c-4c74-c178-fc9b4d10b08b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement keras.optimizers (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for keras.optimizers\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyMPa7Wn5Ymy",
        "outputId": "4701b77d-828a-4f86-e201-af1ddc9a587a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# This is a BoW classifier to detect the presence of provider scare quotes in provider clinical notes\n",
        "\n",
        "import pandas as pd\n",
        "import numpy\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import feature_selection\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import seaborn as sn\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "# Import modules for evaluation purposes\n",
        "# Import libraries for predcton\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,roc_auc_score,roc_curve,auc,f1_score\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "%matplotlib inline\n",
        "import random # to perform randomisation of tasks\n",
        "from tqdm.notebook import tqdm # for a continuous progress bar style\n",
        "import time # time module \n",
        "import os # import operating system"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in data\n",
        "gold_standard = pd.read_csv(\"gold_standard_bias_annotation_doc_training.csv\")\n"
      ],
      "metadata": {
        "id": "bLqa2nI-AHYp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mini HW 9/10—Neural classifier —Instructions:\n",
        "Due 10 Dec\n",
        "In this two part HW, you will split your dataset into training/test sets, and then build a neu-\n",
        "ral classifier of your choice. Recommend LSTM but for an extra challenge you can also consider a transformer/RoBERTA if you prefer (esp if you are working with a low resource language). Do this homework in python only. You will need to move to google co-lab if you haven’t already and take\n",
        "advantage of a GPU.\n",
        "\n",
        "1. First, for HW 8, set up your neural model. You will likely want to use the same training/test split\n",
        "as before, but if you make any adjustments make sure to note the reason. You will need to include\n",
        "Glove embeddings as a feature of your LSTM model. Consider the many available tutorials such\n",
        "as this or this. Consider at least two specifications of your neural model, and write up what you\n",
        "did and why.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zYsMrlYE5ZIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split\n",
        "seed = 1234\n",
        "X = gold_standard['Sentence']\n",
        "y = gold_standard['quote_use']\n",
        "#Hyperparameters\n",
        "max_features_model = 3000\n",
        "splits = 5\n",
        "model_name = \"RNN\"\n",
        "#Splitting and model\n",
        "skf = StratifiedKFold(n_splits=splits, random_state=seed, shuffle= True)\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "tok = Tokenizer(num_words=max_words)\n",
        "tok.fit_on_texts(X_train)\n",
        "sequences = tok.texts_to_sequences(X_train)\n",
        "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
        "\n",
        "max_words = 1000\n",
        "max_len = 150\n",
        "\n"
      ],
      "metadata": {
        "id": "RyClY5Ym5e2T"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RNN():\n",
        "    inputs = Input(name='inputs',shape=[max_len])\n",
        "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
        "    layer = LSTM(64)(layer)\n",
        "    layer = Dense(256,name='FC1')(layer)\n",
        "    layer = Activation('relu')(layer)\n",
        "    layer = Dropout(0.5)(layer)\n",
        "    layer = Dense(1,name='out_layer')(layer)\n",
        "    layer = Activation('sigmoid')(layer)\n",
        "    model = Model(inputs=inputs,outputs=layer)\n",
        "    return model"
      ],
      "metadata": {
        "id": "nzrB6wNu25Q3"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNN()\n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEvN5LcM4OgX",
        "outputId": "81c55154-3ee1-4e61-ddcb-3f5319f9a87d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " inputs (InputLayer)         [(None, 150)]             0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, 150, 50)           50000     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 64)                29440     \n",
            "                                                                 \n",
            " FC1 (Dense)                 (None, 256)               16640     \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 256)               0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " out_layer (Dense)           (None, 1)                 257       \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 1)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 96,337\n",
            "Trainable params: 96,337\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(sequences_matrix,y_train,batch_size=128,epochs=10,\n",
        "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqA1cZri4YdD",
        "outputId": "60eec26c-9ef7-4e83-f8de-49486644bb92"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3/3 [==============================] - 5s 559ms/step - loss: 0.6517 - accuracy: 0.2299 - val_loss: 0.4679 - val_accuracy: 0.2660\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - 1s 229ms/step - loss: -0.0099 - accuracy: 0.1658 - val_loss: -2.5021 - val_accuracy: 0.2660\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5516778f90>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sequences = tok.texts_to_sequences(X_test)\n",
        "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
      ],
      "metadata": {
        "id": "EigqzTSj6VFz"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accr = model.evaluate(test_sequences_matrix,y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OuGyWGQ6XpV",
        "outputId": "cb08e13d-778e-4cfc-b3b3-4b48e868eabc"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 25ms/step - loss: -1.8456 - accuracy: 0.1880\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euftZIsA6dBv",
        "outputId": "58e77c42-c204-4ca0-c186-3690d3254459"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set\n",
            "  Loss: -1.846\n",
            "  Accuracy: 0.188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Next, for HW 10, estimate the F1 score and plot a precision/recall curve to compare your neural\n",
        "model specifications to your BOW model in the same plot. Consider using a tutorial (e.g. here).\n",
        "Write up in overleaf what you did and why, and include the plot that compares your neural and\n",
        "BOW models in your write up on overleaf and submit on Canvas.\n"
      ],
      "metadata": {
        "id": "vVViX_EJ5k1d"
      }
    }
  ]
}